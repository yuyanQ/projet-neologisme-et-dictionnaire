{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yuyanq/Desktop/Bureau/M2/UE3 Dictionnaire et néologisme/projet-néologisme/2006-2011/Annee2006/01/\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import os\n",
    "import warnings\n",
    "import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='bs4')\n",
    "\n",
    "path = os.getcwd() + \"/Annee2006/01/\"\n",
    "print(path)\n",
    "#\"Annee2011/01/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path_fichier):\n",
    "    with open(path_fichier, encoding = \"utf8\") as f:\n",
    "        html_doc = f.read()\n",
    "    return BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "def enlever_nombres(texte):\n",
    "    texte = re.sub(\"[0-9]+|[0-9]+[,\\.-][0-9]+\", \"\", texte)\n",
    "    texte = re.sub(\"\\'\\'\", \"\",  texte)\n",
    "    return texte\n",
    "\n",
    "def update_vocabulaire(set_mots, vocabulaire):\n",
    "    dejavu = vocabulaire.intersection(set_mots)\n",
    "    nouveau = set_mots.difference(vocabulaire)\n",
    "    for x in set_mots:\n",
    "        vocabulaire.add(x) # les types du corpus jusqu'à ce texte-là\n",
    "    return vocabulaire, dejavu, nouveau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isupper_lower(mots):\n",
    "    mots_captalise= [mot for mot in mots if mot[0].isupper()==True]\n",
    "    return len(mots_captalise)\n",
    "\n",
    "def get_effectifs (liste_mots):                        \n",
    "    dic_longueurs = {} \n",
    "    for mot in liste_mots:\n",
    "        longueurs = len(mot)\n",
    "        if longueurs not in dic_longueurs:\n",
    "            dic_longueurs[longueurs] = 0\n",
    "        dic_longueurs[longueurs]+= 1 \n",
    "    return dic_longueurs \n",
    "                   \n",
    "def get_taille(dic_longueurs):\n",
    "    liste_effectifs = []\n",
    "    for lon in range(0,31):\n",
    "        if lon in dic_longueurs:\n",
    "            liste_effectifs.append(dic_longueurs[lon])\n",
    "        else:\n",
    "            liste_effectifs.append(0)         \n",
    "    return liste_effectifs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoire_friendly(dico, path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            old_dict = json.load(f)\n",
    "    except:\n",
    "        old_dict = {}\n",
    "\n",
    "    new_dict = {**old_dict, **dico}\n",
    "    for key in new_dict:\n",
    "        new_dict[key] = list(new_dict[key])\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(json.dumps(new_dict, indent=4, ensure_ascii=False))\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path,encoding='utf8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test d'échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fichier = path+\"02_01_2006.xml\"\n",
    "soup = read(path_fichier)\n",
    "#Créons un vocabulaire tiré de ce fichier du texte\n",
    "L = []\n",
    "for d in soup.find_all(\"div\", type=\"article\"):\n",
    "    texte = d.text\n",
    "    texte = enlever_nombres(texte)\n",
    "    mots = re.findall(\"([A-Z][A-Z0-9.]+|[cCdDjJlLmnNsSt]'|[qQ]u'|\\w+[\\w'-]+\\w+|\\w\\w+)\", texte) # regex sans \\S et sans chiffre\n",
    "    L+=mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ütre', 'ôter', 'îlot', 'îles', 'île', 'ëL', 'êtres', 'être', 'êtes', 'événements']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(set(L), reverse=True)[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du corpus et de la dictionnaire Glaff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###préparons notre vocabualire issu du glaff\n",
    "import re, json\n",
    "import glob\n",
    "\n",
    "with open(\"freq_glaff_%i.json\"%cpt, \"w\",encoding = \"utf8\") as w:\n",
    "    w.write(json.dumps(dic_glaff,indent = 4, ensure_ascii = False)) #lisible pour l'humian\n",
    "set_mots_glaff = set(dic_glaff.keys())\n",
    "print(\"Taille Glaff : %i types\"%len(set_mots_glaff))  # type(mot unique) VS token\n",
    "#print(\"Entrée Glaff : %i formes\"%(cpt)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille Glaff : 1082688 types\n"
     ]
    }
   ],
   "source": [
    "with open(\"../freq_glaff_1406857.json\", encoding=\"utf8\") as t:\n",
    "    dic_glaff = json.load(t)\n",
    "\n",
    "set_mots_glaff = set(dic_glaff.keys())\n",
    "print(\"Taille Glaff : %i types\"%len(set_mots_glaff)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement du corpus \n",
    "- tokeniser\n",
    "- sauvegarder les tokens par jour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "articles_par_date = {}\n",
    "\n",
    "for path_fichier in glob.glob(\"Annee2006/*/*\"):\n",
    "    \n",
    "    date = (path_fichier.split(\"/\")[-1])[:-4]\n",
    "    #print(date)\n",
    "    soup = read(path_fichier)\n",
    "    liste_mots_par_jour =[]\n",
    "    for cpt, d in enumerate(soup.find_all(\"div\", type=\"article\")):\n",
    "        texte = d.text\n",
    "        texte = enlever_nombres(texte)\n",
    "        mots = re.findall(\"([A-Z][A-Z0-9.]+|[cCdDjJlLmnNsSt]'|[qQ]u'|\\w+[\\w'-]+\\w+|\\w\\w+)\", texte) #[\\w'-]+\n",
    "        #(\\w+['-]\\w+)+|\\w+)\n",
    "        # \\w+['-]\\w+ : les mots composés\n",
    "        liste_mots_par_jour+=mots\n",
    "        #save the set of words by day\n",
    "    if date in articles_par_date:\n",
    "        articles_par_date[date] = articles_par_date[date].union(set(liste_mots_par_jour))\n",
    "    else:\n",
    "        articles_par_date[date] = set(liste_mots_par_jour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annee2006 a une collection de 357 jours\n",
      "Le premier jour est le 01_02_2006 et le dernier jour est le 31_12_2006\n"
     ]
    }
   ],
   "source": [
    "chemin_mots_Annee=\"./mot_par_jour/mot_articles_par_Annee2006.json\"\n",
    "memoire_friendly(articles_par_date, chemin_mots_Annee)\n",
    "articles_par_date = read_json_file(chemin_mots_Annee)\n",
    "print(\"%s a une collection de %s jours\"%(chemin_mots_Annee[-14:-5],len(articles_par_date)))\n",
    "print(\"Le premier jour est le %s et le dernier jour est le %s\"%(sorted(articles_par_date.keys())[0],sorted(articles_par_date.keys())[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "NB de mots nouveaux minisculs en total : 72065\n"
     ]
    }
   ],
   "source": [
    "#comparer avec le glaff, calculer la proportion de smots qui sont absents du glaff\n",
    "\n",
    "vocabulaire_glaff_update = set(dic_glaff.keys())\n",
    "neologismeMIN = set()\n",
    "proportion_mots_nouveauxMIN = []\n",
    "liste_effectifs = []\n",
    "\n",
    "sorted_date = sorted(articles_par_date.keys(), key=lambda x: datetime.datetime.strptime(x, '%d_%m_%Y'))\n",
    "\n",
    "for date in sorted_date:\n",
    "    #print(date)\n",
    "\n",
    "    set_mots_jour = set(articles_par_date[date])\n",
    "    vocabulaire_glaff_update, dejavu, nouveau = update_vocabulaire(set_mots_jour, vocabulaire_glaff_update)\n",
    "    # cas 2 + cas 3 # nouveau\n",
    "    mots_dans_glaff = set(set_mots_jour).intersection(set(dic_glaff.keys())) # cas 1 + cas 4\n",
    "\n",
    "    ##cas 1:min + glaff ##cas 4:maj + glaff\n",
    "    Lower_mots_dans_glaff = [mot.lower()for mot in mots_dans_glaff] # min+maj glaff\n",
    "    cas1_min_glaff = mots_dans_glaff.intersection(Lower_mots_dans_glaff)\n",
    "    cas4_maj_glaff = mots_dans_glaff.difference(Lower_mots_dans_glaff)\n",
    "\n",
    "    ##cas 2:min + pas glaff ##cas 3:maj + pas glaff\n",
    "    Lower_mots_abscent = [mot.lower()for mot in nouveau] # min+maj glaff\n",
    "    cas2_min_non_glaff =  nouveau.intersection(Lower_mots_abscent)\n",
    "    cas3_maj_non_glaff = nouveau.difference(Lower_mots_abscent)\n",
    "   \n",
    "    neologismeMIN = neologismeMIN.union(cas2_min_non_glaff)\n",
    "    liste_jour = [date,len(nouveau),len(cas2_min_non_glaff)]\n",
    "    \n",
    "    proportion_mots_nouveauxMIN.append((len(cas2_min_non_glaff)/len(set_mots_jour)))\n",
    "    liste_effectifs.append(liste_jour)\n",
    "print(\"-\"*50)\n",
    "print(\"NB de mots nouveaux minisculs en total :\",len(neologismeMIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annee2006 a une collection de 72065 néologismes\n"
     ]
    }
   ],
   "source": [
    "chemin_neo_Annee=\"/Users/yuyanq/Desktop/Bureau/M2/UE3 Dictionnaire et néologisme/projet-néologisme/1999-2003/neo/neo.json\"\n",
    "\n",
    "neologismeDico = {\"Annee2006\": neologismeMIN}\n",
    "memoire_friendly(neologismeDico, chemin_neo_Annee)\n",
    "print(\"%s a une collection de %s néologismes\"%(\"Annee2006\",len(neologismeMIN)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webster's Unabridged English Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille Webster : 86036 types\n",
      "Taille neo : 1980 types\n"
     ]
    }
   ],
   "source": [
    "###préparons notre vocabualire anglaise\n",
    "# https://github.com/adambom/dictionary\n",
    "\n",
    "\n",
    "with open(\"../dictionary.json\", encoding=\"utf8\") as t:\n",
    "    dic_Webster = json.load(t)\n",
    "\n",
    "set_mots_Webster = set([mot.lower() for mot in dic_Webster.keys()])\n",
    "print(\"Taille Webster : %i types\"%len(set_mots_Webster))  # type(mot unique) VS token\n",
    "\n",
    "neo_emprunt = list(set(neologismeMIN).intersection(set_mots_Webster))\n",
    "print(\"Taille neo : %i types\"%len(neo_emprunt))  # type(mot unique) VS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_neo_emprunt_Annee=\"/Users/yuyanq/Desktop/Bureau/M2/UE3 Dictionnaire et néologisme/projet-néologisme/1999-2003//neo_emprunt/neo_emprunt.json\"\n",
    "neologisme_enprunt_Dico = {\"Annee2006\": neo_emprunt}\n",
    "memoire_friendly(neologisme_enprunt_Dico, chemin_neo_emprunt_Annee)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
